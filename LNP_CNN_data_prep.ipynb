{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "## Main directories\n",
    "**data** - 3 channels: 0 = celltracker (CT); 1 = LNP; 2 = brightfield (BF)\n",
    "* 8 wells and two FOVs per well (one well excluded due to missing data)\n",
    "* data for cells until they're tracking ends, i.e. they go too close to the edge of the FOV  \n",
    "* well C03 is excluded as it has missing data - see below\n",
    "\n",
    "**gfp** - 1 channel = GFP for the corresponding images in **data**\n",
    "\n",
    "## Statistics to use when standardizing or normalizing the data\n",
    "StatsRecorder below from http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n",
    "saved as 'statsrecorder.py'\n",
    "* cell_stats - min, max, mean and sd (for each input channels)\n",
    "* gfp values based on inner cut-outs (92 x 92 pixels)\n",
    "* gfp_stats - min, max, mean, sd, gfp_thresh\n",
    "* where gfp_thresh - the threshold for a positive gfp target (from control wells)\n",
    "\n",
    "## Directories for modelling data\n",
    "**images_train**, **images_test**, **gfp_train** and **gfp_test**\n",
    "* models based on data for the first 20 time points to predict GFP at the final time point (t=72)\n",
    "* data is a subset of the main directories for those cells that made it to the end (and only their first 20 time points)\n",
    "* cells are shuffled into the training and test directories (all 20 images for each cell going into one or the other folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stretching jupyter notebook view\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import re\n",
    "import glob\n",
    "import os, shutil\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import statsrecorder as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc functions\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall image size\n",
    "im_shape0 = 2154\n",
    "im_shape1 = 2554\n",
    "\n",
    "# path to images\n",
    "image_folder = '/mnt/external-images-pvc/AZ/LNP/AssayPlate_Greiner_#781091/'\n",
    "\n",
    "# offset = 96 -> tile size of 192 x 192 (divides nicely by two for when max pooling)\n",
    "offset = 96\n",
    "xydim = offset * 2\n",
    "\n",
    "# warp matrix for BF registration\n",
    "warp_matrix = np.load('warp.npy')\n",
    "\n",
    "# CREATE MAIN DATA FOLDERS (with data across all 72 time points)\n",
    "data_dir = '/scratch-shared/phil/LNP/LNP_data_09/data'\n",
    "os.makedirs(data_dir)\n",
    "gfp_dir = '/scratch-shared/phil/LNP/LNP_data_09/gfp'\n",
    "os.makedirs(gfp_dir)\n",
    "\n",
    "# well and FOV IDs\n",
    "# well C03 excluded due to missing data\n",
    "c_files = ['C04_F001', 'C04_F002', 'C05_F001', 'C05_F002',\n",
    "          'E03_F001', 'E03_F002', 'E04_F001', 'E04_F002', 'E05_F001', 'E05_F002',\n",
    "          'G03_F001', 'G03_F002', 'G04_F001', 'G04_F002', 'G05_F001', 'G05_F002']\n",
    "\n",
    "# control files - no mRNA added to LNPs; used for determining threshold for a +ive GFP response\n",
    "control_files = ['G03_F001', 'G03_F002', 'G04_F001', 'G04_F002', 'G05_F001', 'G05_F002']\n",
    "\n",
    "# training and test folders for models based on t=1:20\n",
    "t_stop = 20\n",
    "images_train_dir = '/scratch-shared/phil/LNP/LNP_data_09/images_train'\n",
    "os.makedirs(images_train_dir)\n",
    "images_test_dir = '/scratch-shared/phil/LNP/LNP_data_09/images_test'\n",
    "os.makedirs(images_test_dir)\n",
    "\n",
    "# folders for the targets\n",
    "gfp_train_dir = '/scratch-shared/phil/LNP/LNP_data_09/gfp_train'\n",
    "os.makedirs(gfp_train_dir)\n",
    "gfp_test_dir = '/scratch-shared/phil/LNP/LNP_data_09/gfp_test'\n",
    "os.makedirs(gfp_test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add cell images to data_dir and gfp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_file in c_files:\n",
    "    s = c_file.split('_')\n",
    "    f = glob.glob(image_folder + '/*' + s[0] + '*' + s[1] + '*C07*')\n",
    "    files = sorted(f, key=natural_keys)\n",
    "    # path to tracking_coords\n",
    "    coords_file = 'tracking_coords/' + c_file + '.npy'\n",
    "    coords = np.load(coords_file)\n",
    "    for cell_nr in range(coords.shape[0]):\n",
    "        for i, coord in enumerate(coords[cell_nr]):\n",
    "            # if go outside window stop collecting data for cell\n",
    "            if (coord[1] - offset < 0 or \n",
    "                coord[1] - offset + int(warp_matrix[1,2]) < 0 or\n",
    "                coord[1] + offset > im_shape0 or \n",
    "                coord[1] + offset + int(warp_matrix[1,2]) > im_shape0 or\n",
    "                coord[0] - offset < 0 or\n",
    "                coord[0] - offset + int(warp_matrix[0,2]) < 0 or\n",
    "                coord[0] + offset > im_shape1 or\n",
    "                coord[0] + offset + int(warp_matrix[0,2]) > im_shape1):\n",
    "                break\n",
    "            # tp = time point in format '_T0001' to '_T0072'\n",
    "            tp = files[i][-26:-20]\n",
    "            # 0 = CT; 1 = LNP; 2 = BF; 3 = GFP\n",
    "            f0 = glob.glob(image_folder + '/*' + s[0] + tp + '*' + s[1] + '*' + 'C07.tif')\n",
    "            f1 = glob.glob(image_folder + '/*' + s[0] + tp + '*' + s[1] + '*' + 'C06.tif')\n",
    "            f2 = glob.glob(image_folder + '/*' + s[0] + tp + '*' + s[1] + '*' + 'C09.tif')\n",
    "            f3 = glob.glob(image_folder + '/*' + s[0] + tp + '*' + s[1] + '*' + 'C05.tif')\n",
    "            im0 = cv2.imread(f0[0], -1)\n",
    "            crop0 = im0[(coord[1] - offset):(coord[1] + offset), (coord[0] - offset):(coord[0] + offset)]\n",
    "            im1 = cv2.imread(f1[0], -1)\n",
    "            crop1 = im1[(coord[1] - offset):(coord[1] + offset), (coord[0] - offset):(coord[0] + offset)]\n",
    "            im2 = cv2.imread(f2[0], -1)\n",
    "            crop2 = im2[(coord[1] - offset + int(warp_matrix[1, 2])):\n",
    "                        (coord[1] + offset + int(warp_matrix[1, 2])),\n",
    "                        (coord[0] - offset + int(warp_matrix[0, 2])):\n",
    "                        (coord[0] + offset + int(warp_matrix[0, 2]))]\n",
    "            im_all = np.reshape(crop0, (xydim, xydim, 1))\n",
    "            im_all = np.append(im_all, np.reshape(crop1, (xydim, xydim, 1)), axis=2)\n",
    "            im_all = np.append(im_all, np.reshape(crop2, (xydim, xydim, 1)), axis=2)\n",
    "            im3 = cv2.imread(f3[0], -1)\n",
    "            crop3 = im3[(coord[1] - offset):(coord[1] + offset), (coord[0] - offset):(coord[0] + offset)]\n",
    "            gfp = np.reshape(crop3, (xydim, xydim, 1))\n",
    "            np.save(data_dir + '/' + c_file + tp + '_cell_'+ str(cell_nr) + '.npy', im_all)\n",
    "            np.save(gfp_dir + '/' + c_file + tp + '_cell_'+ str(cell_nr) + '.npy', gfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfp stats:\n",
      "[      0.         4484517.          487273.34219269  630943.22057337\n",
      "  126950.        ]\n",
      "no. training cells = 602\n",
      "positives = 387\n",
      "negatives = 215\n",
      "no. test cells = 172\n",
      "positives = 123\n",
      "negatives = 49\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.8 # 80% of data to training\n",
    "train_y = []\n",
    "train_name = []\n",
    "test_y = []\n",
    "test_name = []\n",
    "\n",
    "# collecting gfp statistics from training data\n",
    "# to allow for various standradization/normalization methods for the target when in regression mode\n",
    "mystats = sr.StatsRecorder()\n",
    "minmax = np.zeros(2)\n",
    "\n",
    "lo = 48 # for centred cropping of gfp images\n",
    "hi = 144\n",
    "\n",
    "for c_file in c_files:\n",
    "    # pulling out only those cells that made it to the end\n",
    "    f_cell = glob.glob(gfp_dir + '/' + c_file + '*' + '_T0072' + '*')\n",
    "    f_cell = sorted(f_cell, key = natural_keys)\n",
    "    for i, f in enumerate(f_cell):\n",
    "        im = np.load(f)\n",
    "        im2 = im[lo:hi, lo:hi, 0] # centred crop\n",
    "        ff = f.split('gfp/')[1]\n",
    "        ff = ''.join(ff.split('_T0072'))\n",
    "        if np.random.random() <= train_split:\n",
    "            train_y = np.append(train_y, np.sum(im2))\n",
    "            train_name.extend([ff])\n",
    "            \n",
    "            if np.sum(im2) < minmax[0]:\n",
    "                minmax[0] = np.sum(im2)\n",
    "            if np.sum(im2) > minmax[1]:\n",
    "                minmax[1] = np.sum(im2)\n",
    "            mystats.update(np.sum(im2))\n",
    "        else:\n",
    "            test_y = np.append(test_y, np.sum(im2))\n",
    "            test_name.extend([ff])\n",
    "\n",
    "# threshold for gfp computed across all time points for all control cells (for preprocessing)\n",
    "gfp_thresh = 0\n",
    "for t_file in control_files:\n",
    "    files = glob.glob(gfp_dir + '/' + t_file + '*')\n",
    "    for i, f in enumerate(files):\n",
    "        im = np.load(f)\n",
    "        im2 = im[lo:hi, lo:hi, 0] # centred crop\n",
    "        if np.sum(im2) > gfp_thresh:\n",
    "            gfp_thresh = np.sum(im2)\n",
    "\n",
    "gfp_stats = np.zeros(5)\n",
    "gfp_stats[0] = minmax[0]\n",
    "gfp_stats[1] = minmax[1]\n",
    "gfp_stats[2] = mystats.mean\n",
    "gfp_stats[3] = mystats.std\n",
    "gfp_stats[4] = gfp_thresh\n",
    "\n",
    "np.save('/scratch-shared/phil/LNP/LNP_data_09/gfp_stats.npy', gfp_stats)\n",
    "\n",
    "print('gfp stats:')\n",
    "print(gfp_stats)\n",
    "\n",
    "print('no. training cells =', len(train_y))\n",
    "print('positives =', sum(train_y > gfp_thresh))\n",
    "print('negatives =', sum(train_y <= gfp_thresh))\n",
    "\n",
    "print('no. test cells =', len(test_y))\n",
    "print('positives =', sum(test_y > gfp_thresh))\n",
    "print('negatives =', sum(test_y <= gfp_thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding target data to gfp train and test directories\n",
    "for name in train_name:\n",
    "    s = name.split('_')\n",
    "    f_cell = glob.glob(gfp_dir + '/' + s[0] + '_' + s[1] + '_T0072' + '_cell_' + s[3])\n",
    "    im = np.load(f_cell[0])\n",
    "    im2 = im[lo:hi, lo:hi, 0] # centred crop\n",
    "    gfp = np.sum(im2)\n",
    "    np.save(gfp_train_dir + '/' + s[0] + '_' + s[1] + '_T0072' + '_cell_' + s[3], gfp)\n",
    "\n",
    "for name in test_name:\n",
    "    s = name.split('_')\n",
    "    f_cell = glob.glob(gfp_dir + '/' + s[0] + '_' + s[1] + '_T0072' + '_cell_' + s[3])\n",
    "    im = np.load(f_cell[0])\n",
    "    im2 = im[lo:hi, lo:hi, 0] # centred crop\n",
    "    gfp = np.sum(im2)\n",
    "    np.save(gfp_test_dir + '/' + s[0] + '_' + s[1] + '_T0072' + '_cell_' + s[3], gfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell stats for CT:\n",
      "[0.00000000e+00 3.21440000e+04 8.50557418e+00 5.95228701e+00]\n",
      "cell stats for LNP:\n",
      "[0.00000000e+00 1.07580000e+04 1.36255645e+00 1.19130562e+00]\n",
      "cell stats for BF:\n",
      "[   0.         4196.           76.11380371    7.76618654]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing to get cell images in correct format for VGG16 base model\n",
    "tpoints = '_T0001'\n",
    "for i in range(2, 10):\n",
    "    tpoints = np.append(tpoints, '_T000' + str(i))\n",
    "for i in range(10, t_stop + 1):\n",
    "    tpoints = np.append(tpoints, '_T00' + str(i))\n",
    "\n",
    "# computing cell level stats for cell images (for preprocessing)\n",
    "mystats = sr.StatsRecorder() # first channel\n",
    "minmax = np.zeros((3,2))\n",
    "\n",
    "# calculate min and max for training data (correcting for differences in dynamic ranges)\n",
    "for name in train_name:\n",
    "    for i, tp in enumerate(tpoints):\n",
    "        s = name.split('_')\n",
    "        f_cell = glob.glob(data_dir + '/' + s[0] + '_' + s[1] + tp + '_cell_' + s[3])\n",
    "        im = np.load(f_cell[0])\n",
    "        im_new = im.reshape(-1, im.shape[-1])\n",
    "        for j in range(3):\n",
    "            if np.min(im_new[:, j]) < minmax[j, 0]:\n",
    "                minmax[j, 0] = np.min(im_new[:, j])\n",
    "            if np.max(im_new[:, j]) > minmax[j, 1]:\n",
    "                minmax[j, 1] = np.max(im_new[:, j])\n",
    "\n",
    "# calculate mean once data is on the right scale for VGG16 input\n",
    "for name in train_name:\n",
    "    for i, tp in enumerate(tpoints):\n",
    "        s = name.split('_')\n",
    "        f_cell = glob.glob(data_dir + '/' + s[0] + '_' + s[1] + tp + '_cell_' + s[3])\n",
    "        im = np.load(f_cell[0])\n",
    "        im = im.astype('float32')\n",
    "        im_new = im\n",
    "        for j in range(3):\n",
    "            im_new[:, :, j] = 255 * (im[:, :, j] - minmax[j, 0]) / minmax[j, 1]\n",
    "        \n",
    "        im_new2 = im_new.reshape(-1, im_new.shape[-1])\n",
    "        mystats.update(im_new2)\n",
    "        \n",
    "cell_stats = np.zeros((3,4))\n",
    "for j in range(3):\n",
    "    cell_stats[j, 0] = minmax[j, 0]\n",
    "    cell_stats[j, 1] = minmax[j, 1]\n",
    "\n",
    "cell_stats[:, 2] = mystats.mean\n",
    "cell_stats[:, 3] = mystats.std\n",
    "\n",
    "np.save('/scratch-shared/phil/LNP/LNP_data_09/cell_stats.npy', cell_stats)\n",
    "\n",
    "print('cell stats for CT:')\n",
    "print(cell_stats[0])\n",
    "print('cell stats for LNP:')\n",
    "print(cell_stats[1])\n",
    "print('cell stats for BF:')\n",
    "print(cell_stats[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding cell image data to train and test directories\n",
    "# and getting data into VGG16 input format (0-255 scale with training data channel means subtracted)\n",
    "for name in train_name:\n",
    "    # 5-fold cross-validation: random alocation to fold for each training cell\n",
    "    fold = random.choice(['fold1', 'fold2', 'fold3', 'fold4', 'fold5'])\n",
    "    \n",
    "    for i, tp in enumerate(tpoints):\n",
    "        s = name.split('_')\n",
    "        f_cell = glob.glob(data_dir + '/' + s[0] + '_' + s[1] + tp + '_cell_' + s[3])\n",
    "        im = np.load(f_cell[0])\n",
    "        im = im.astype('float32')\n",
    "        im_new = im\n",
    "        for j in range(3):\n",
    "            im_new[:, :, j] = 255 * ((im[:, :, j] - minmax[j, 0]) / minmax[j, 1]) - cell_stats[j, 2]\n",
    "        \n",
    "        np.save(images_train_dir + '/' + fold + '_' + s[0] + '_' + s[1] + tp + '_cell_' + s[3], im_new)\n",
    "\n",
    "for name in test_name:\n",
    "    for i, tp in enumerate(tpoints):\n",
    "        s = name.split('_')\n",
    "        f_cell = glob.glob(data_dir + '/' + s[0] + '_' + s[1] + tp + '_cell_' + s[3])\n",
    "        im = np.load(f_cell[0])\n",
    "        im = im.astype('float32')\n",
    "        im_new = im\n",
    "        for j in range(3):\n",
    "            im_new[:, :, j] = 255 * (im[:, :, j] - minmax[j, 0]) / minmax[j, 1]\n",
    "            # clipping any test images with a wider range than training images\n",
    "            im_new[:, :, j] = np.clip(im_new[:, :, j], minmax[j, 0], minmax[j, 1])\n",
    "            im_new[:, :, j] = im_new[:, :, j] - cell_stats[j, 2]\n",
    "        \n",
    "        np.save(images_test_dir + '/' + s[0] + '_' + s[1] + tp + '_cell_' + s[3], im_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
